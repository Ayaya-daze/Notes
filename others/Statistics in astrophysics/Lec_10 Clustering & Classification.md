
>Clustering : 是指对一些数据对象根据某些特征分类, 让一个类中的数据特征尽可能接近. 这是一种 unsupervised learning
>Classification: 在进行了 Clustering 或者已有分类之后, 对于一个新的数据, 确定它属于哪一类, 这是一种 supervised learning

实际上天文上很多时候就在做各种各样的分类, 很多时候要从数据里面学习

>一个例子是对恒星的分类: 例如根据恒星的光谱性质分类, 后来发现这些光谱的分类能对应恒星的温度等物理性质
>很多时候一些奇怪的标签其实是历史原因
>第二个例子也是恒星的例子: HR diagram
>在温度和亮度的参数空间可以发现所谓恒星的主序带, 这个图上可以让我们理解恒星的物理演化过程
>Hubble Sequence: 对星系的分类, 后来从这个分类中同样发现了一些星系演化的信息

这种分类的动机是:


# Clustering

## K - means method
目标: 将数据分为 $K$ 类, 带中心 $\{\mu_k\}$
方法是最小化各个偏离的差
$$\min\sum\sum ||x-\mu_k||^2$$
这实际上是一个迭代过程: 将每个数据分至距离它最近的中心, 然后重新根据数据的属类计算数据的中心, 在这个新的中心下再次分类, 这样一直迭代

这种分类的结果给出所谓 voronoi diagram: 对参数空间进行了划分

例如在高维空间判断密度时, 可以使用这种方法

>但是这种分类也有问题: 收敛的结果依赖于初值, 同样也对 outlier 敏感, 并且这个距离没有做合适的归一化, 这严重依赖于参数的标度(某个坐标的数据巨大的话会显著影响最小化)
>这些都是 K - means 的限制
>1. 需要先验地选择合适的 $K$ 
>2. 对初值敏感
>3. 距离函数的要求很高

一个更好的方法是 Hierarchical clustering

## Hierarchical clustering
想法是把每个数据都当作一个单独的类, 然后通过某个过程把靠近的类聚成同一个类

实现是逐点地对类对 $(C_k,C_{k'})$ 进行判断它们的距离
single linkage $d_{\min}(C_k,C_{k'})=$
complete linkage
average linkage

然后取距离最短的 pair , 将他们归为同一类
这种过程会给出一个树形图 dendrogram
实际上重复这个过程会得到一个平凡的单类, 正确的做法是在其中做一个 cut off

>在选择 single - linkage 距离时这种算法也被称为 friends of friends 算法
>这等价于截断一个最小生成树

在使用 complete linkage 时会得到一个 compact 的分类

一个例子是在 SDSS 中分类星系团, 寻找一些结构, 这会得到一个最小生成树, 这里面会包含数据的一些拓扑结构
在选择了一个截断后, 并且可以要求: 一个真正的 group 里面应该包含多个点, 就会得到一些分类

另一个例子是 gamma 射线暴事件的分类

>性质


DBSCAN
在有噪声的情况下通过密度估计做聚类





能够自适应地找出 outlier

>故算法的流程为
>1. 选择一个任意点
>2. 检查这个点的邻域: 如果邻域内的点很少, 则视作噪声, 反之, 认为这是一个core, 并放置一个新的类
>3. 通过加入其他邻居点得到更大的类

并且这个过程并不依赖初值选择: 对于点的分类是自适应的


一个例子是找星团: 这种情况不能只依靠坐标信息分类: 一个星团的组分很有可能被抛射到很远处. 这时候应该使用 DBSCAN 方法

性质:


>总结
>Clustering 实际上就是把点分类的过程


# Classification
目标: 对于新的数据点, 判断它属于哪一类
数据
坐标
实际上这也是一个概率问题: 计算新的数据属于各个类的概率, 直接的做法是用 Bayes 公式

如何判断这种分类是否有效?

一种判断是考虑 completeness : 考虑正确分类数据的占比

另一种方法是 contamination : 


一种 naive 的 Bayes 公式: 假定各个数据是独立的
$$p()=\prod p(\mid)$$
这样, 似然为

所以实际上做的是最大似然
$$\max$$

对于似然有不同的选择

例如 Gauss 型的似然
$$p(|) = \frac{1}{\sqrt{2\pi}\sigma_{y_k}} \exp$$

每个 feature 都是独立的 gauss 分布, 最后只用全部乘起来


一个例子是 RR Lyrae 变星 的选择
一个做法是在 HR diagram 中考虑, 

但是效果不是很好

一种改进方法是放宽 feature 独立分布的假设, 认为它们有一个联合分布
比如一个联合 Gauss 分布

对于协方差矩阵有很多选择
LDA
QDA


另一种分类器是之前的 k - 近邻分类器

## k - 近邻
假设是如果两个点很近的话, 似然也应该相近

对一个点分类的方式是对邻居点打上标签, 

>性质

污染显著减小, 但是完备度也降低了

报菜名:
SVM 支持向量机
找一个超平面分隔两个数据类的点

Kernelization SVM KSVM
将低维的数据映到高维去, 高维的超平面会被映到低维的一个复杂的边界


决策树 
从一个包含所有数据的结点开始
通过一个决策边界把数据分成子集

性质


随机森林


性质

# Dimension reduction

有时候并不希望维度很高, 会选择一些降维的方法
要求

## PCA
主成分分析
目标: 选择一些正交的方向
直接使用线性代数


## SOM
自组织映射



